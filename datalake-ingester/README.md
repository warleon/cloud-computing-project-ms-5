# DataLake Ingester - ETL Pipeline

Pipeline ETL implementado con Python que extrae datos desde 3 bases de datos diferentes, los transforma a formato JSON Lines y los carga en **Amazon S3** con particionamiento por fecha, listo para ser catalogado por **AWS Glue** y consultado con **Amazon Athena**.

## ğŸ¯ DescripciÃ³n del Componente

Este componente implementa el pipeline ETL completo del DataLake:

### **Extract (ExtracciÃ³n)**
- ConexiÃ³n a MySQL, PostgreSQL y MongoDB
- ExtracciÃ³n de todas las tablas/colecciones de cada base de datos
- Lectura completa de datos sin filtros

### **Transform (TransformaciÃ³n)**
- ConversiÃ³n automÃ¡tica de tipos incompatibles con JSON:
  - `Decimal` â†’ `float`
  - `datetime`/`date` â†’ `string` (formato ISO 8601)
  - `bytes` â†’ `string` (UTF-8)
- Formato JSON Lines (NDJSON): un objeto JSON por lÃ­nea
- ValidaciÃ³n de datos antes de carga

### **Load (Carga)**
- Subida a Amazon S3 usando boto3
- Particionamiento automÃ¡tico por fecha: `year=YYYY/month=MM/day=DD/`
- 3 buckets S3 diferentes (uno por microservicio)
- Uso de IAM Role para autenticaciÃ³n AWS

## ğŸš€ Despliegue en AWS

### Infraestructura AWS Implementada:

**Amazon S3 - 3 Buckets Creados:**
- `raw-ms1-data-bgc` â†’ Datos de MySQL (MS1)
- `raw-ms2-data-bgc` â†’ Datos de PostgreSQL (MS2)
- `raw-ms3-data-bgc` â†’ Datos de MongoDB (MS3)

**EC2 Ubuntu 22.04:**
- IAM Role asignado: `LabRole` (permisos S3, Glue, Athena)
- Docker instalado y configurado
- 3 contenedores de ingester ejecutÃ¡ndose:
  - `ingesta01-mysql` â†’ Extrae MySQL â†’ S3
  - `ingesta02-postgresql` â†’ Extrae PostgreSQL â†’ S3
  - `ingesta03-mongodb` â†’ Extrae MongoDB â†’ S3

**Datos Ingestados:**
- 9 tablas/colecciones procesadas
- Datos en formato JSON Lines (NDJSON)
- Estructura de particionamiento implementada
- Datos listos para Glue Crawlers

## ğŸ—ï¸ Arquitectura

```
Bases de Datos                Ingesters (Docker)           Amazon S3
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MySQL     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  ingesta01-mysql â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ raw-ms1-data-bgc/   â”‚
â”‚  (MS1)      â”‚              â”‚  (Python+boto3)  â”‚         â”‚   users/            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   orders/           â”‚
                                                           â”‚   products/         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚                     â”‚
â”‚ PostgreSQL  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ingesta02-postgresâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ raw-ms2-data-bgc/   â”‚
â”‚  (MS2)      â”‚              â”‚  (Python+boto3)  â”‚         â”‚   customers/        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   invoices/         â”‚
                                                           â”‚   payments/         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚                     â”‚
â”‚  MongoDB    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ ingesta03-mongodbâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ raw-ms3-data-bgc/   â”‚
â”‚  (MS3)      â”‚              â”‚  (Python+boto3)  â”‚         â”‚   inventory/        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   shipments/        â”‚
                                                           â”‚   suppliers/        â”‚
                                                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ CaracterÃ­sticas Implementadas

- âœ… **Multi-database support**: MySQL, PostgreSQL y MongoDB integrados
- âœ… **ConversiÃ³n automÃ¡tica de tipos**: Decimalâ†’float, datetimeâ†’ISO string, bytesâ†’string
- âœ… **Formato JSON Lines (NDJSON)**: Un objeto JSON por lÃ­nea, optimizado para Athena
- âœ… **Particionamiento por fecha**: `year=YYYY/month=MM/day=DD/` para queries eficientes
- âœ… **ContainerizaciÃ³n**: 3 contenedores independientes, uno por fuente de datos
- âœ… **ConfiguraciÃ³n por environment**: Variables de entorno con `.env` files
- âœ… **IAM Role authentication**: Usa credenciales del EC2 automÃ¡ticamente (sin hardcoding)
- âœ… **Manejo de errores**: Logging detallado y manejo de excepciones
- âœ… **EjecuciÃ³n one-shot**: Los contenedores ejecutan y terminan (no quedan corriendo)

## ğŸ“Š Resultados de Ingesta

### Datos Procesados y Cargados a S3:

**Bucket: `raw-ms1-data-bgc`** (MySQL)
- `users/` â†’ 10 registros
- `orders/` â†’ 15 registros
- `products/` â†’ 12 registros
- **Total MS1**: 37 registros

**Bucket: `raw-ms2-data-bgc`** (PostgreSQL)
- `customers/` â†’ 5 registros
- `invoices/` â†’ 8 registros
- `payments/` â†’ 6 registros
- **Total MS2**: 19 registros

**Bucket: `raw-ms3-data-bgc`** (MongoDB)
- `inventory/` â†’ 6 documentos
- `shipments/` â†’ 7 documentos
- `suppliers/` â†’ 5 documentos
- **Total MS3**: 18 documentos

**Gran Total**: 74 registros ingestados exitosamente a S3

### Estructura de Archivos en S3:
```
s3://raw-ms1-data-bgc/
  â””â”€â”€ users/
      â””â”€â”€ year=2025/
          â””â”€â”€ month=10/
              â””â”€â”€ day=05/
                  â””â”€â”€ data.json (JSON Lines format)
```

## âš™ï¸ ConfiguraciÃ³n Implementada

### Variables de Entorno
Los ingesters estÃ¡n configurados con las siguientes variables en el archivo `.env`:

```bash
# AWS Configuration (implementada)
AWS_DEFAULT_REGION=us-east-1
AWS_ACCESS_KEY_ID=        # VacÃ­o (usa IAM Role de EC2)
AWS_SECRET_ACCESS_KEY=    # VacÃ­o (usa IAM Role de EC2)
AWS_SESSION_TOKEN=        # VacÃ­o (usa IAM Role de EC2)

# S3 Buckets Configuration (3 buckets creados)
S3_BUCKET_MS1=raw-ms1-data-bgc    # Para datos MySQL
S3_BUCKET_MS2=raw-ms2-data-bgc    # Para datos PostgreSQL
S3_BUCKET_MS3=raw-ms3-data-bgc    # Para datos MongoDB

# MySQL Database Connection (MS1)
MYSQL_HOST=mysql-db              # Nombre del contenedor
MYSQL_PORT=3306                  # Puerto interno
MYSQL_USER=root
MYSQL_PASSWORD=********
MYSQL_DATABASE=testdb

# PostgreSQL Database Connection (MS2)
POSTGRES_HOST=postgres-db        # Nombre del contenedor
POSTGRES_PORT=5432               # Puerto interno
POSTGRES_USER=postgres
POSTGRES_PASSWORD=********
POSTGRES_DATABASE=testdb

# MongoDB Database Connection (MS3)
MONGO_HOST=mongo-db              # Nombre del contenedor
MONGO_PORT=27017                 # Puerto interno
MONGO_USER=admin
MONGO_PASSWORD=********
MONGO_DATABASE=testdb
MONGO_AUTH_SOURCE=admin

# Ingestion Configuration
INGESTION_INTERVAL=3600
ENABLE_PARTITIONING=true
LOG_LEVEL=INFO
```

## ğŸ”„ EjecuciÃ³n de Ingesters

### Modo de EjecuciÃ³n Implementado

Los 3 ingesters se ejecutan como **contenedores one-shot** (ejecutan y terminan):

```bash
# Estado de ejecuciÃ³n
Container: ingesta01-mysql        â†’ Ejecuta extracciÃ³n MySQL â†’ Sube a S3 â†’ Termina (Exit 0)
Container: ingesta02-postgresql   â†’ Ejecuta extracciÃ³n PostgreSQL â†’ Sube a S3 â†’ Termina (Exit 0)
Container: ingesta03-mongodb      â†’ Ejecuta extracciÃ³n MongoDB â†’ Sube a S3 â†’ Termina (Exit 0)
```

### Logs de EjecuciÃ³n

```bash
# Ver logs de ingesta exitosa
docker logs ingesta01-mysql
# Output: "ExtracciÃ³n completada: 3 tablas procesadas â†’ raw-ms1-data-bgc"

docker logs ingesta02-postgresql
# Output: "ExtracciÃ³n completada: 3 tablas procesadas â†’ raw-ms2-data-bgc"

docker logs ingesta03-mongodb
# Output: "ExtracciÃ³n completada: 3 colecciones procesadas â†’ raw-ms3-data-bgc"
```

### VerificaciÃ³n en S3

```bash
# Verificar datos en buckets S3
aws s3 ls s3://raw-ms1-data-bgc/ --recursive
aws s3 ls s3://raw-ms2-data-bgc/ --recursive
aws s3 ls s3://raw-ms3-data-bgc/ --recursive

# Ejemplo de salida:
# 2025-10-05 users/year=2025/month=10/day=05/data.json
# 2025-10-05 orders/year=2025/month=10/day=05/data.json
# 2025-10-05 products/year=2025/month=10/day=05/data.json
```

## Estructura de datos en S3

Los datos se almacenan con la siguiente estructura:
```
s3://bucket-name/
  â””â”€â”€ table_name/
      â””â”€â”€ year=2025/
          â””â”€â”€ month=10/
              â””â”€â”€ day=04/
                  â””â”€â”€ data.json
```

Cada archivo `data.json` contiene datos en formato JSON Lines (un objeto JSON por lÃ­nea):
```json
{"id": 1, "name": "John", "email": "john@example.com"}
{"id": 2, "name": "Jane", "email": "jane@example.com"}
```

## ConversiÃ³n de Tipos

El ingester convierte automÃ¡ticamente:
- `Decimal` â†’ `float`
- `datetime`/`date` â†’ `string` (ISO 8601)
- `bytes` â†’ `string` (UTF-8)

Esto asegura que AWS Glue Crawler infiera correctamente los tipos de datos.

## Troubleshooting

### Error: "DB_TYPE y S3_BUCKET son requeridos"
- Verifica que el archivo `.env` exista y tenga las variables correctas
- Revisa que `docker-compose.yml` estÃ© cargando el archivo `.env`

### Error de conexiÃ³n a la base de datos
- Verifica que el host sea correcto (nombre del contenedor en Docker)
- AsegÃºrate de que las bases de datos estÃ©n corriendo
- Revisa las credenciales en `.env`

### Error de permisos en S3
- Verifica que el EC2 tenga un IAM Role con permisos S3
- Revisa que los buckets existan y estÃ©n en la misma regiÃ³n
