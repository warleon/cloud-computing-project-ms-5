# DataLake Ingester

Scripts Python que extraen datos desde bases de datos (MySQL, PostgreSQL, MongoDB) y los suben a Amazon S3 en formato JSON Lines, listos para ser catalogados por AWS Glue y consultados con Athena.

## ğŸ¯ PropÃ³sito

El ingester es el componente ETL (Extract, Transform, Load) del DataLake:
- **Extract**: Conecta a las bases de datos y extrae todas las tablas/colecciones
- **Transform**: Convierte tipos de datos problemÃ¡ticos (Decimal, datetime) a formatos JSON vÃ¡lidos
- **Load**: Sube los datos a S3 con particionamiento por fecha

## ğŸ—ï¸ Arquitectura

```
Bases de Datos                Ingesters (Docker)           Amazon S3
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MySQL     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  ingesta01-mysql â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ raw-ms1-data-bgc/   â”‚
â”‚  (MS1)      â”‚              â”‚  (Python+boto3)  â”‚         â”‚   users/            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   orders/           â”‚
                                                           â”‚   products/         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚                     â”‚
â”‚ PostgreSQL  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ingesta02-postgresâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ raw-ms2-data-bgc/   â”‚
â”‚  (MS2)      â”‚              â”‚  (Python+boto3)  â”‚         â”‚   customers/        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   invoices/         â”‚
                                                           â”‚   payments/         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚                     â”‚
â”‚  MongoDB    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ ingesta03-mongodbâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ raw-ms3-data-bgc/   â”‚
â”‚  (MS3)      â”‚              â”‚  (Python+boto3)  â”‚         â”‚   inventory/        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   shipments/        â”‚
                                                           â”‚   suppliers/        â”‚
                                                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ CaracterÃ­sticas

- âœ… **Multi-database**: Soporte para MySQL, PostgreSQL y MongoDB
- âœ… **Type conversion**: Convierte automÃ¡ticamente Decimalâ†’float, datetimeâ†’ISO string
- âœ… **JSON Lines format**: Datos en NDJSON, formato Ã³ptimo para Athena
- âœ… **Date partitioning**: Estructura `year=YYYY/month=MM/day=DD/` para queries eficientes
- âœ… **Containerized**: Cada ingester corre en su propio contenedor Docker
- âœ… **Environment-based**: ConfiguraciÃ³n mediante variables de entorno (.env)
- âœ… **IAM Role support**: Usa credenciales del EC2 automÃ¡ticamente

## ğŸ“‹ Pre-requisitos

- EC2 con Docker y Docker Compose instalados
- IAM Role con permisos S3 PutObject (ej: LabRole)
- 3 buckets S3 ya creados
- Bases de datos corriendo y accesibles (ver `ms-databases/`)

## ConfiguraciÃ³n

### Variables de Entorno

1. Copia el archivo de ejemplo:
```bash
cp .env.example .env
```

2. Configura las variables en `.env`:

```bash
# AWS Configuration
AWS_DEFAULT_REGION=us-east-1

# S3 Buckets Configuration
S3_BUCKET_MS1=raw-ms1-data-bgc
S3_BUCKET_MS2=raw-ms2-data-bgc
S3_BUCKET_MS3=raw-ms3-data-bgc

# MySQL Database (MS1)
MYSQL_HOST=mysql-test-db
MYSQL_PORT=3306
MYSQL_USER=root
MYSQL_PASSWORD=rootpassword
MYSQL_DATABASE=testdb

# PostgreSQL Database (MS2)
POSTGRES_HOST=postgres-test-db
POSTGRES_PORT=5432
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgrespassword
POSTGRES_DATABASE=testdb

# MongoDB Database (MS3)
MONGO_HOST=mongo-test-db
MONGO_PORT=27017
MONGO_USER=admin
MONGO_PASSWORD=adminpassword
MONGO_DATABASE=testdb
MONGO_AUTH_SOURCE=admin

# Ingestion Configuration
INGESTION_INTERVAL=3600
ENABLE_PARTITIONING=true
LOG_LEVEL=INFO
```

## Uso

### Con Docker Compose (recomendado)

Ejecutar todos los ingesters:
```bash
docker-compose up -d
```

Ver logs:
```bash
docker logs ingesta01-mysql -f
docker logs ingesta02-postgresql -f
docker logs ingesta03-mongodb -f
```

Detener ingesters:
```bash
docker-compose down
```

### EjecuciÃ³n Manual

Ejecutar un ingester especÃ­fico:
```bash
# MySQL
export DB_TYPE=mysql
export DB_HOST=mysql-test-db
export DB_PORT=3306
export DB_USER=root
export DB_PASSWORD=rootpassword
export DB_NAME=testdb
export S3_BUCKET=raw-ms1-data-bgc
export TABLES=users,orders,products

python ingester.py
```

## Estructura de datos en S3

Los datos se almacenan con la siguiente estructura:
```
s3://bucket-name/
  â””â”€â”€ table_name/
      â””â”€â”€ year=2025/
          â””â”€â”€ month=10/
              â””â”€â”€ day=04/
                  â””â”€â”€ data.json
```

Cada archivo `data.json` contiene datos en formato JSON Lines (un objeto JSON por lÃ­nea):
```json
{"id": 1, "name": "John", "email": "john@example.com"}
{"id": 2, "name": "Jane", "email": "jane@example.com"}
```

## ConversiÃ³n de Tipos

El ingester convierte automÃ¡ticamente:
- `Decimal` â†’ `float`
- `datetime`/`date` â†’ `string` (ISO 8601)
- `bytes` â†’ `string` (UTF-8)

Esto asegura que AWS Glue Crawler infiera correctamente los tipos de datos.

## Troubleshooting

### Error: "DB_TYPE y S3_BUCKET son requeridos"
- Verifica que el archivo `.env` exista y tenga las variables correctas
- Revisa que `docker-compose.yml` estÃ© cargando el archivo `.env`

### Error de conexiÃ³n a la base de datos
- Verifica que el host sea correcto (nombre del contenedor en Docker)
- AsegÃºrate de que las bases de datos estÃ©n corriendo
- Revisa las credenciales en `.env`

### Error de permisos en S3
- Verifica que el EC2 tenga un IAM Role con permisos S3
- Revisa que los buckets existan y estÃ©n en la misma regiÃ³n
